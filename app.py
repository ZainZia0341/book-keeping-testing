# libraries import
import os
# from langchain import hub
from dotenv import load_dotenv
# from langchain_community.vectorstores import Chroma
# from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools.retriever import create_retriever_tool
from langchain_core.messages import BaseMessage
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import tool

from langgraph.graph.message import add_messages
from langgraph.graph import END, StateGraph, START
from langgraph.prebuilt import ToolNode
# from langgraph.prebuilt import tools_condition
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

from pydantic import BaseModel, Field
from typing import Annotated, Sequence, Literal, Union, Dict, Any
from typing_extensions import TypedDict
from dateutil import parser as date_parser

from pymongo import MongoClient
from bson import ObjectId
import json
import time
# import datetime
# import ast

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import traceback

from sqlalchemy.exc import OperationalError

# Loading Env file
load_dotenv()

# file import
# from pinecone_reuse import get_pinecone_index

# from chromadb_reuse import get_chromadb_index

# LangSmith for Error Tracing
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = os.environ.get("LANGCHAIN_API_KEY")

# Setting Environment Variables
os.environ["GOOGLE_API_KEY"] = os.environ.get("GOOGLE_API_KEY")
DB_URI = os.environ.get("Postgres_sql_URL") 
MONGO_URI = os.environ.get("MONGODB_URI")
BEDROCK_CREDENTIALS_PROFILE_NAME = os.getenv("BEDROCK_CREDENTIALS_PROFILE_NAME", "default")

from langchain_groq import ChatGroq

llm = ChatGroq(
    model="llama-3.3-70b-specdec", # "llama-3.1-70b-versatile", # "llama-3.2-90b-text-preview",  # "llama-3.3-70b-specdec", # "llama3-8b-8192"
    groq_api_key=os.environ.get("GROQ_API_KEY"),
    temperature=0,
    max_tokens=None,
)

# from langchain_aws import ChatBedrock
# from langchain.llms.bedrock import Bedrock
# from langchain_community.chat_models import BedrockChat

# main_agent_llm = ChatBedrock(
#         model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",  # Replace with your specific model ID
#         credentials_profile_name=BEDROCK_CREDENTIALS_PROFILE_NAME,
#         region = "us-east-1",
#         model_kwargs = {
#         "temperature": 0,
#     }
#     )

# Initialize LLM
# llm = ChatGoogleGenerativeAI(
#     model="gemini-1.5-pro",
#     temperature=0,
#     max_tokens=None,
# )

from mongodb_reuse import get_mongodb_vector_store

# retriever = get_pinecone_index()
# retriever = get_chromadb_index()
vector_store = get_mongodb_vector_store()

from pprint import pprint

# Assuming `retriever` is the MongoDBAtlasVectorSearch object
pprint(vars(vector_store))

print("MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM ", vector_store)


retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.2},
)



# Define a global variable for user_id
user_id_global = ""

# Create the retriever tool
retriever_tool = create_retriever_tool(
        retriever,
        name="LedgerIQ_FAQs",
        description="This tool retrieves answers from the Ledger IQ FAQ dataset. It is used to handle questions about the app's features, functionality, usage instructions, and company details. Example queries include: 'How do I fix a transaction error in Ledger IQ?' and 'How do I handle personal expenses in Ledger IQ?'"
)


@tool
def Mongodb_tool(query_str: Union[str, dict]) -> list:
    """
    This tool executes MongoDB queries generated by the query generator node to retrieve matching documents from the transactions collection.
    """
    print("---MONGODB_TOOL: Executing query---")

    if isinstance(query_str, str):
        print(f"Received query_str as string: {query_str}")
        try:
            query_dict = json.loads(query_str)
            print(f"Parsed query_dict from string: {query_dict}")
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse JSON query string: {query_str}\nError: {e}")
    elif isinstance(query_str, dict):
        query_dict = query_str
        print(f"Received query_str as dict: {query_dict}")
    else:
        raise TypeError("query_str must be either a string or a dictionary.")

    # Convert values that look like ISO datetimes to Python datetime objects
    # to allow $gte / $lte queries on date properly
    def convert_values(d):
        for key, value in d.items():
            if isinstance(value, dict):
                convert_values(value)
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, dict):
                        convert_values(item)
            else:
                if key == 'user' and isinstance(value, str):
                    try:
                        d[key] = ObjectId(value)
                    except Exception as e:
                        print(f"Error converting 'user' to ObjectId: {e}")
                elif key == 'date' and isinstance(value, str):
                    try:
                        d[key] = date_parser.isoparse(value)
                    except Exception as e:
                        print(f"Error converting 'date' to datetime: {e}")
                elif isinstance(value, str):
                    try:
                        d[key] = date_parser.isoparse(value)
                    except ValueError:
                        pass
        return d

    query_dict = convert_values(query_dict)
    print(f"Converted query_dict: {query_dict}")

    # ──────────────────────────────────────────────────────────────────────────
    # DYNAMICALLY ADD THE USER FILTER FROM THE GLOBAL VARIABLE user_id_global
    # ──────────────────────────────────────────────────────────────────────────
    global user_id_global
    if user_id_global:
        # Insert user filter so final query includes "user": ObjectId(user_id_global)
        query_dict["user"] = ObjectId(user_id_global)
        print(f"Added user filter to query: {query_dict['user']}")

    print(f"Converted query_dict: {query_dict}")

    # Connect to MongoDB
    # Ensure this is correctly set
    print("Connecting to MongoDB...", MONGO_URI)
    if not MONGO_URI:
        raise ValueError("MONGO_URI not set in environment variables.")

    client = MongoClient(MONGO_URI)
    db = client["mathew_data"]  # Replace with your actual DB name if different
    print(f"Connected to MongoDB database: {db.name}")

    # Execute the query in 'transactions' collection
    transactions_collection = db["transactions"]
    print(f"Executing query in 'transactions' collection: {query_dict}")
    results_cursor = transactions_collection.find(query_dict)

    # Convert cursor to a list of dicts
    results = list(results_cursor)
    print("Retrieved documents from MongoDB.", results)
    print(f"Retrieved {len(results)} documents from MongoDB.")

    # Close the connection
    client.close()
    print("Closed MongoDB connection.")

    # Return the documents
    return results

tools = [retriever_tool]

tools_for_agent2 = [Mongodb_tool]

def generate_query_str(state):
    """
    This tool converts the user's natural language query into a MongoDB query dictionary for retrieving transaction data. It ensures accurate queries aligned with the database structure and content.

    Purpose:
    Generates MongoDB queries as Python dictionaries for filtering data from the transactions collection in the mathew_data database.
    Handles date ranges, transaction types (CREDIT/DEBIT), amounts, and merchant details.
    Note: The user field is injected separately from the global variable user_id_global and should not be included in the generated query.

    Example Database Document:
    json
    {
        "_id": ObjectId("6772c9650ad791a776bdf2ee"),
        "user": ObjectId("6724a7ae270a38bc33cbcf2e"),
        "merchant": {
            "id": "mch_12y7t59Rw4Yp5h6ZvLiTmR",
            "name": "Fifth Third Bank"
        },
        "date": ISODate("2024-10-29T00:00:00Z"),
        "description": "Mortgage Payment",
        "entryType": "CREDIT",
        "amount": 50.32
    }

    When to Use:
    Use this tool for transaction-related queries, such as retrieving data, identifying trends, or performing financial calculations. Example queries include:

    What transactions were made between October 1, 2024, and October 31, 2024?
    Generated Query:
    {
        "date": {
            "$gte": datetime.datetime(2024, 10, 1, 0, 0),
            "$lte": datetime.datetime(2024, 10, 31, 0, 0)
        }
    }

    List all debit transactions above $500 in 2024.
    Generated Query:
    {
        "date": {
            "$gte": datetime.datetime(2024, 1, 1, 0, 0),
            "$lte": datetime.datetime(2024, 12, 31, 0, 0)
        },
        "entryType": "DEBIT",
        "amount": {"$gte": 500}
    }
    
    Args:
        state (dict): The current conversation state, including user messages, previous tool invocations, and context.
    
    Returns:
        dict: A MongoDB query dictionary for filtering transactions.
    """

    # llm_query_generator = ChatBedrock(
    #     model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",  # Replace with your specific model ID
    #     credentials_profile_name=BEDROCK_CREDENTIALS_PROFILE_NAME,
    #     region = "us-east-1",
    #     model_kwargs = {
    #     "temperature": 0,
    # }
    # )

    # llm = llm_query_generator

    print("---CALL (generate_query_str) Node---")
    messages = state["messages"]
    model = llm.bind_tools(tools_for_agent2)  # Bind only Mongodb_tool
    response = model.invoke(messages)
    return {"messages": [response]}



class AgentState(TypedDict, total=False):
    messages: Annotated[Sequence[BaseMessage], add_messages]



def agent(state):
    """
    1. Retrieve Node (retrieve)
        When to Use:
        Use this node for questions about Ledger IQ's features, functionality, or company-related information. It retrieves answers from a predefined FAQ dataset.
        Example Questions:
        What is Ledger IQ?
        How do I send an invoice with Ledger IQ?
    2. Query Generator Node (generate_query_str)
        When to Use:
        Use this node for financial data queries that require constructing a MongoDB query. The user's question will typically involve financial analysis, trends, or calculations, such as revenue, expenses, profit margins, ROI, or budgeting.
        Example Questions:
        How profitable is my company this quarter compared to last quarter?
        What is the total revenue and expenses for the last six months?
    3. General Questions Node (general_questions_node)
        When to Use:
        Use this node for casual or general-purpose queries that do not require specific financial or FAQ-related answers. This includes greetings, jokes, or creative content generation.
        Example Questions:
        Hello, how are you?
        Tell me a joke.

    **Args**:
        state (dict): The current conversation state, including user messages, previous tool invocations, and context.

    **Returns**:
        dict: The updated state with the agent's decision only do not write anything else in response just route the question to there specific node/tool.
    """
    print("---CALL AGENT---")
    messages = state["messages"]
    print(f"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ", messages)
    # model = main_agent_llm
    model = llm
    model = model.bind_tools(tools)
    response = model.invoke(messages)
    # time.sleep(3)
    # We return a list, because this will get added to the existing list
    print("GGGGGGGGGGGGGGGGGGGGGGG ", response)
    return {"messages": [response]}


def generate(state):
    print("---GENERATE---")
    messages = state["messages"]
    question = messages[0].content
    last_message = messages[-1]

    docs = last_message.content

    print("TTTTTTTTTTTTTTTTTT ", docs)
    print("YYYYYYYYYYYYYYYYYY ", question)
    print("UUUUUUUUUUUUUUUUUUUUUUU", retriever)
    prompt = PromptTemplate(
        template="""The AI should provide conversational and engaging responses, answering user questions clearly and encouraging further dialogue. At the end of each response, it should offer additional assistance or suggest ways to help. Avoid using phrases like 'I don't know' or 'I only know this according to my database.'
        Here is the retrieved document: \n\n {docs} \n\n
        Here is the user question: {question} \n
        """,
        input_variables=["docs", "question"],
    )

    # llm_generate  = ChatBedrock(
    #     model_id="amazon.titan-text-premier-v1:0",  # Replace with your specific model ID
    #     credentials_profile_name=BEDROCK_CREDENTIALS_PROFILE_NAME,
    #     region = "us-east-1",
    #     model_kwargs = {
    #     "temperature": 0,
    # }
    # )

    # llm = llm_generate

    # Chain
    rag_chain = prompt | llm | StrOutputParser()

    # Run
    # time.sleep(3)
    response = rag_chain.invoke({"docs": docs, "question": question})
    return {"messages": [response]}



def generate_finance_answer(state):
    print("---GENERATE FINANCE INFO---")
    messages = state["messages"]
    user_question = messages[0].content  # User's original question
    tool_message = messages[-1]  # Assuming ToolMessage is the last message
    financial_data = tool_message.content  # Raw data as string
    print("State data:", state)

    print(f"User Question: {user_question}")
    print(f"Financial data received: {financial_data}")
    
    if not financial_data:
        print("No financial data found for the given query.")
        return {"messages": [AIMessage(content="No financial data found for the given query.")]}
    
    # Convert the financial_data list to a string representation for the prompt
    financial_data_str = "\n".join([str(doc) for doc in financial_data])
    
    # Define the prompt for the LLM with input variables
    llm_prompt = """
    You are a financial analyst assistant. Based on the transaction data retrieved from MongoDB and the user's question, perform the necessary calculations and provide a comprehensive financial response specific to the user's request.

    **User Question:**
    {user_question}

    **Transaction Data from MongoDB:**
    {financial_data}

   generate_finance_answer
    -----------------------
    **Purpose**:
      - Generate financial insights based on MongoDB transaction data and the user's query.
      - Perform necessary calculations such as totals, averages, profit, ROI, cash flow, and trends.

    **Calculations**:
      1. **Summing Transactions**:
         - Formula: Total Amount = sum(Transaction Amounts)
         - Example: Sum all income or expenses within a date range.

      2. **Averaging Transactions**:
         - Formula: Average Amount = Total Amount / Number of Transactions
         - Example: Calculate the average monthly expense.

      3. **Profit and ROI**:
         - Profit: Profit = Total Income - Total Expenses
         - ROI: ROI (%) = (Net Profit / Investment) × 100
         - Example: Determine net profit or ROI for a specific investment.

      4. **Cash Flow Analysis**:
         - Formula: Net Cash Flow = Total Inflows - Total Outflows
         - Example: Evaluate financial health by analyzing inflows vs. outflows.

      5. **Trends and Growth Rates**:
         - Formula: Growth Rate (%) = (Current Period Amount - Previous Period Amount) / Previous Period Amount × 100
         - Example: Identify increases or decreases in income or expenses over time.

      6. **Expense Breakdown**:
         - Fixed Costs: Sum transactions labeled as fixed costs (e.g., rent, salaries).
         - Variable Costs: Sum transactions labeled as variable costs (e.g., marketing, utilities).

    **Response**:
      - Generate a concise and clear financial answer, directly addressing the user's query.
      - Include actionable recommendations or insights if applicable.
"""

    # Create the PromptTemplate with input variables
    prompt = PromptTemplate(
        template=llm_prompt,
        input_variables=["user_question", "financial_data"]
    )

    # financial_llm = ChatBedrock(
    #     model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",  # Replace with your specific model ID
    #     credentials_profile_name=BEDROCK_CREDENTIALS_PROFILE_NAME,
    #     region = "us-east-1",
    #     model_kwargs = {
    #     "temperature": 0,
    # }
    # )

    # llm = financial_llm

    # Chain the prompt with the LLM and output parser
    query_chain = prompt | llm | StrOutputParser()

    # Generate the financial report by passing the required variables
    try:
        generated_report = query_chain.invoke({
            "user_question": user_question,
            "financial_data": financial_data_str
        })
        print(f"LLM Generated Finance Report: {generated_report}")
    except Exception as e:
        print(f"Error in generating finance report: {e}")
        return {"messages": [AIMessage(content="There was an error generating your financial report. Please try again later.")]}
    
    return {"messages": [AIMessage(content=generated_report)]}


def custom_condition(state):
    """
    Custom routing condition based on the user's query content.
    Routes to 'generate_query_str' for personal finance queries,
    and 'retrieve' for company information queries.
    """
    question = state["messages"][0].content.lower()
    finance_keywords = [
        "my account", "transactions", "credited", "debits",
        "balance", "profit", "loss", "budget", "income", 
        "expenses", "cash flow", "investment", "roi",
        "financial metrics", "revenue", "expense", "profit margin",
        "net profit", "expense ratio", "revenue growth", "return on sales"
    ]
    general_keywords = [
        "hello", "hi", "hey", "whatsup", "what's up", "how are you",
        "write a poem", "tell me a joke", "good morning", "good evening"
    ]
    if any(keyword in question for keyword in general_keywords):
        print("Routing to general_questions_node based on general keywords.")
        return "general_questions_node"
    elif any(keyword in question for keyword in finance_keywords):
        print("Routing to generate_query_str based on finance keywords.")
        return "generate_query_str"
    else:
        print("Routing to LedgerIQ_FAQs based on company info keywords.")
        return "retrieve"
    
def general_questions_node(state) -> dict:
    """
    Handles general user inquiries such as greetings, jokes, or poems without using any tools.
    
    Args:
        state (dict): The current conversation state, including user messages.
    
    Returns:
        dict: The updated state with the generated general response.
    """
    print("---GENERAL QUESTIONS NODE: Generating response---")
    messages = state["messages"]
    # user_message = messages[0].content
    print(" RRRRRRRRRRRRRR ", messages[0].content)
    print(" RRRRRRRRRRRRR RRRR ", messages[-1].content)
    last_human_message = None

    # Iterate in reverse order to find the last HumanMessage
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            last_human_message = msg
            break  # Stop as soon as we find the last HumanMessage

    # Access the content of the last HumanMessage
    if last_human_message:
        user_message = last_human_message.content
        print(f"Last Human Message: {user_message}")
    else:
        print("No HumanMessage found.")
    print("OOOOOOOOOOOOOOOOOOOOOOOOOOOOOO ", user_message)
    # Define the prompt template for general responses
    prompt = PromptTemplate(
        template="""
            You are a friendly and helpful AI assistant.

            Respond to the user's message in a natural and engaging way without using external tools. If the user asks for general content generation tasks like writing poems or jokes, politely inform them that you specialize in financial data and encourage them to ask something related to the app or their finances.

            Example Question: "Hi"

            AI Response: "Hello! I'm an AI assistant specialized in financial data. Feel free to ask something related to the app or your financial data."

            User Message: "{user_message}"

            AI Response:
        """,
        input_variables=["user_message"]
    )

    # llm_general_case  = ChatBedrock(
    #     model_id="anthropic.claude-3-5-haiku-20241022-v1:0",  # Replace with your specific model ID
    #     credentials_profile_name=BEDROCK_CREDENTIALS_PROFILE_NAME,
    #     region = "us-east-1",
    #     model_kwargs = {
    #     "temperature": 0,
    # }
    # )

    # llm = llm_general_case
    
    
    # Chain the prompt with the LLM and output parser
    response_chain = prompt | llm | StrOutputParser()
    
    try:
        ai_response = response_chain.invoke({"user_message": user_message})
        print(f"Generated General Response: {ai_response}")
    except Exception as e:
        print(f"Error in general_questions_node: {e}")
        ai_response = "I'm sorry, but I couldn't process your request. Could you please try again?"
    
    return {"messages": [AIMessage(content=ai_response)]}


# Define a new graph
workflow = StateGraph(AgentState)

# Define the nodes we will cycle between
workflow.add_node("agent", agent)  # agent
retrieve_node = ToolNode([retriever_tool])
workflow.add_node("retrieve", retrieve_node)  # retrieval
mongodb_tool = ToolNode([Mongodb_tool])
workflow.add_node("mongodb_tool_node", mongodb_tool)
workflow.add_node("general_questions_node", general_questions_node)
workflow.add_node("generate_query_str", generate_query_str)  # Generates query_str
workflow.add_node("generate", generate)  # Generating a response after we know the documents are relevant
workflow.add_node("generate_finance_answer", generate_finance_answer)  # Generating a response after we know the documents are relevant
# Call agent node to decide to retrieve or not
workflow.add_edge(START, "agent")

# Decide whether to retrieve
# workflow.add_conditional_edges(
#     "agent1",
#     # Assess agent decision
#     tools_condition,
#     {
#         # Translate the condition outputs to nodes in our graph
#         "mongodb_tool_node": "mongodb_tool_node",
#         "tools": "retrieve",
#         END: "retrieve",
#     },
# )

# Decide whether to retrieve using custom_condition
workflow.add_conditional_edges(
    "agent",
    custom_condition,  # Use custom condition instead of tools_condition
    {
        "generate_query_str": "generate_query_str",
        "retrieve": "retrieve",
        "general_questions_node": "general_questions_node"
    },
)


# Update edges for MongoDB path
workflow.add_edge("generate_query_str", "mongodb_tool_node")  # After routing to MongoDB tool 
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)                     # After generating response, end
workflow.add_edge("mongodb_tool_node", "generate_finance_answer")  # After MongoDB query, generate finance answer
workflow.add_edge("generate_finance_answer", END)      # After finance answer, end
workflow.add_edge("general_questions_node", END)


# ____________________________________________________ Display Graph ____________________________________________________ #

def get_langGraph_image_flow():
    from IPython.display import Image, display
    try:
        graph = workflow.compile()
        display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
    except Exception:
        # This requires some extra dependencies and is optional
        pass


# ________________________________________________________________________________________________________________________ #


async def drop_prepared_statements(conn):
    """
    Drops any prepared statements to clean up the database connection.

    Args:
        conn: The database connection
    """
    async with conn.cursor() as cursor:
        await cursor.execute("DEALLOCATE ALL;")

async def execute_workflow(input_message: str, thread_id: str, user_id: str) -> dict:
    global user_id_global
    user_id_global = user_id
    """
    Executes the workflow with the given input message, thread ID, and user ID.

    Args:
        input_message (str): The user's input message
        thread_id (str): The unique identifier for the conversation thread

    Returns:
        dict: The result of the workflow execution
    """
    try:

        async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:
            async with checkpointer.conn.transaction():
                await drop_prepared_statements(checkpointer.conn)
            await checkpointer.setup()
            graph = workflow.compile(checkpointer=checkpointer)
            # Include user_id in the config
            config = {"configurable": {"thread_id": thread_id, "user_id": user_id, "recursion_limit": "4"}}
            res = await graph.ainvoke({"messages": [("human", input_message)]}, config)
            return res
    except Exception as e:
        print(f"Error during workflow execution: {e}")
        return {"error": str(e)}
    


# ____________________________________________________ FAST API ____________________________________________________ #

app = FastAPI()

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Frontend origin
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all headers
    expose_headers=["*"]
)

class UserRequest(BaseModel):
    user_id: str
    message: str
    thread_id: str



@app.post("/agent")
async def agent_endpoint(req: UserRequest):
    user_id = req.user_id
    message = req.message
    thread_id = req.thread_id

    try:
        # 1. Execute main workflow
        response_dict = await execute_workflow(
            input_message=message,
            thread_id=thread_id,
            user_id=user_id
        )
        return {"result": response_dict}

    except OperationalError as op_err:
        # 2. Specifically handle OperationalError
        err_str = str(op_err).lower()

        # Check for "the connection is closed"
        if "the connection is closed" in err_str:
            print("Database connection was closed unexpectedly.")
            # Optionally: Attempt to reconnect or retry
            return JSONResponse(
                status_code=503,
                content={
                    "message": (
                        "Database connection was closed unexpectedly. Please try again shortly or contact support."
                    )
                }
            )

        # Check for "SSL connection has been closed unexpectedly"
        elif "ssl connection has been closed unexpectedly" in err_str:
            print("SSL connection issue with the database.")
            return JSONResponse(
                status_code=503,
                content={
                    "message": (
                        "SSL connection issue with the database. Please check your network or contact support."
                    )
                }
            )

        # Catch-all for other OperationalError variants
        else:
            print("An unknown OperationalError occurred.")
            traceback.print_exc()
            return JSONResponse(
                status_code=503,
                content={
                    "message": f"A database error occurred: {op_err}"
                }
            )

    except Exception as e:
        # 3. Handle any other unhandled exceptions
        print("----- ERROR OCCURRED -----")
        traceback.print_exc()  # For debugging; remove in production if needed
        return JSONResponse(
            status_code=500,
            content={
                "message": f"An unexpected error occurred: {str(e)}"
            }
        )